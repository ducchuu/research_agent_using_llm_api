{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"Quantum Gravity\"\n",
    "\n",
    "max_results = 10\n",
    "query = \"+\".join(search_term.lower().split())\n",
    "url = f\"http://export.arxiv.org/api/query?search_query=all:{query}max_results={max_results}\"\n",
    "url\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_arxiv_xml(xml_string):\n",
    "    # Define namespaces\n",
    "    namespaces = {\n",
    "        \"atom\": \"http://www.w3.org/2005/Atom\",\n",
    "        \"opensearch\": \"http://a9.com/-/spec/opensearch/1.1/\",\n",
    "        \"arxiv\": \"http://arxiv.org/schemas/atom\"\n",
    "    }\n",
    "\n",
    "    root = ET.fromstring(xml_string)\n",
    "\n",
    "    feed = {\n",
    "        \"title\": root.find(\"atom:title\", namespaces).text if root.find(\"atom:title\", namespaces) is not None else \"\",\n",
    "        \"id\": root.find(\"atom:id\", namespaces).text if root.find(\"atom:id\", namespaces) is not None else \"\",\n",
    "        \"updated\": root.find(\"atom:updated\", namespaces).text if root.find(\"atom:updated\", namespaces) is not None else \"\",\n",
    "        \"totalResults\": root.find(\"opensearch:totalResults\", namespaces).text if root.find(\"opensearch:totalResults\", namespaces) is not None else \"\",\n",
    "        \"startIndex\": root.find(\"opensearch:startIndex\", namespaces).text if root.find(\"opensearch:startIndex\", namespaces) is not None else \"\",\n",
    "        \"itemsPerPage\": root.find(\"opensearch:itemsPerPage\", namespaces).text if root.find(\"opensearch:itemsPerPage\", namespaces) is not None else \"\",\n",
    "        \"entries\": []\n",
    "    }\n",
    "\n",
    "    for entry in root.findall(\"atom:entry\", namespaces):\n",
    "        authors = [author.find(\"atom:name\", namespaces).text for author in entry.findall(\"atom:author\", namespaces)]\n",
    "        categories = [category.attrib.get(\"term\", \"\") for category in entry.findall(\"atom:category\", namespaces)]\n",
    "\n",
    "        entry_data = {\n",
    "            \"id\": entry.find(\"atom:id\", namespaces).text,\n",
    "            \"updated\": entry.find(\"atom:updated\", namespaces).text,\n",
    "            \"published\": entry.find(\"atom:published\", namespaces).text,\n",
    "            \"title\": entry.find(\"atom:title\", namespaces).text,\n",
    "            \"summary\": entry.find(\"atom:summary\", namespaces).text.strip(),\n",
    "            \"authors\": authors,\n",
    "            \"comment\": entry.find(\"arxiv:comment\", namespaces).text if entry.find(\"arxiv:comment\", namespaces) is not None else \"\",\n",
    "            \"journal_ref\": entry.find(\"arxiv:journal_ref\", namespaces).text if entry.find(\"arxiv:journal_ref\", namespaces) is not None else \"\",\n",
    "            \"doi\": entry.find(\"arxiv:doi\", namespaces).text if entry.find(\"arxiv:doi\", namespaces) is not None else \"\",\n",
    "            \"links\": {link.attrib.get(\"title\", \"default\"): link.attrib.get(\"href\", \"\") for link in entry.findall(\"atom:link\", namespaces)},\n",
    "            \"primary_category\": entry.find(\"arxiv:primary_category\", namespaces).attrib.get(\"term\", \"\") if entry.find(\"arxiv:primary_category\", namespaces) is not None else \"\",\n",
    "            \"categories\": categories\n",
    "        }\n",
    "\n",
    "        feed[\"entries\"].append(entry_data)\n",
    "\n",
    "    return feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = parse_arxiv_xml(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[\"entries\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"entries\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[paper[\"categories\"] for paper in data[\"entries\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arxiv_papers(search_term: str) -> dict:\n",
    "    max_results = 10\n",
    "    query = \"+\".join(search_term.lower().split())\n",
    "    for char in list('()\" '):\n",
    "        if char in query:\n",
    "            raise ValueError(f\"Cannot have character: '{char}' in query: {query}\")\n",
    "    url = f\"http://export.arxiv.org/api/query?search_query=all:{query}&max_results={max_results}\"\n",
    "    resp = requests.get(url)\n",
    "    print(resp)\n",
    "    data = parse_arxiv_xml(resp.text)\n",
    "    print(json.dumps([{\"title\": paper[\"title\"], \"categories\": paper[\"categories\"]} for paper in data[\"entries\"]], indent=2))\n",
    "    return data, resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, resp = get_arxiv_papers(\"BEC BCS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"entries\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"entries\"][0][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download paper as tex file\n",
    "\n",
    "import requests\n",
    "import tarfile\n",
    "import io\n",
    "\n",
    "# URL of the .tar.gz file\n",
    "url = \"https://arxiv.org/src/1003.4735v1\"\n",
    "\n",
    "# Download the file into memory\n",
    "resp = requests.get(url, stream=True)\n",
    "if resp.status_code == 200:\n",
    "    tar_gz_data = io.BytesIO(resp.content)  # Load resp content into memory\n",
    "    \n",
    "    # Open the tar.gz file in memory\n",
    "    with tarfile.open(fileobj=tar_gz_data, mode=\"r:gz\") as tar:\n",
    "        # Iterate over each file in the archive\n",
    "        for member in tar.getmembers():\n",
    "            if member.isfile():  # Skip directories\n",
    "                file_obj = tar.extractfile(member)\n",
    "                if file_obj:\n",
    "                    try:\n",
    "                        content = file_obj.read().decode(\"utf-8\")  # Read and decode file\n",
    "                        print(f\"\\n--- {member.name} ---\\n\")\n",
    "                        print(content[:500])  # Print first 500 characters\n",
    "                    except UnicodeDecodeError:\n",
    "                        print(f\"Skipping binary file: {member.name}\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {resp.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[paper[\"links\"] for paper in data[\"entries\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import base64\n",
    "import httpx\n",
    "\n",
    "# Load and encode the PDF\n",
    "pdf_url = \"https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf\"\n",
    "pdf_data = base64.standard_b64encode(httpx.get(pdf_url).content).decode(\"utf-8\")\n",
    "\n",
    "# Send to Claude\n",
    "client = anthropic.Anthropic()\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"document\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": \"application/pdf\",\n",
    "                        \"data\": pdf_data\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What are the key findings in this document?\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to print state?\n",
    "\n",
    "from typing import Annotated\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "memory = MemorySaver()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# The first argument is the unique node name\n",
    "# The second argument is the function or object that will be called whenever\n",
    "# the node is used.\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "    snapshot = graph.get_state(config)\n",
    "    return snapshot\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    snapshot = stream_graph_updates(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot.*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dumps(snapshot.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot.values[\"messages\"][-1].content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
